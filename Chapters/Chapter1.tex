\chapter{Introduction}
Fitting is the process of generating a 3D model of a face from a 2D image. There are different approaches on how to do that. The gravis group of the University of Basel has developed an MCMC (Markov Chain Monte Carlo) algorithm that makes random changes to a 3DMM (3D Morphable Model) and accepts the proposal face whenever the new 3D face has a greater probability on the image that has to be fitted than the previous one (Apart from a certain amount of randomness which allows steps in less probable regions). For this Thesis, we use the popular Basel Face Model 2017 \cite{BFM2017}. To allow the algorithm to determine the likelihood of a 3D facial proposal relative to a given 2D face, the algorithm's evaluator needs to know, which pixels to include in the probability calculation. Therefore we have to label each pixel whether it is part of the face and should be regarded by the evaluator, or if it is background and hasn't any importance for the construction of the 3D Model. Nirkin et al \cite{nirkin2018_faceswap} claim, that this is possible with a standard Fully Convolutional Network (FCN).

\section{Artificial neural networks}
The idea of Artificial Neural Networks was heavily influenced by biology. These Networks consist of a variety of neurons which are grouped in layers. The way each neuron works is very simple. It takes multiple inputs of varying strength from other neurons, sums them up and decides depending on the sum whether it should send a stimulus itself and if so, in which strength. Each layer is somehow connected to the next layer. Some layers are fully connected (each neuron of a layer is connected to every other neuron in the next layer) while others are convolutional. Convolutional means that a neuron only gets input of its neighbors in a previous layer. There are many different architectures which mainly differ in the number of layers, number of neurons per layer and the interconnectivity of the neurons. A classical Convolutional Neural Network (CNN) is depicted in Figure \ref{fig:chap1:classicalCNN}.\\
\\
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{Figures/chap1/classicalCNN.JPG}
	\caption{An example of an classical Convolutional Neural Network (CNN). After a certain number of convolutions, a pooling layer extracts the most important information of the image and writes it into the next layer. That is why the picture is getting smaller and smaller until just a vector is left. In our case (FCN) the picture is up sampled again to the original size.}
	\label{fig:chap1:classicalCNN}
\end{figure}

Already in 1943 Warren McCulloch and Walter Pitts \cite{mcculloch} showed that even simple networks of this kind can simulate every possible logical formula. For this, they used a neuron model that consisted of simple logic gates and could only process only binary input and output signals.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Figures/chap1/bio_vs_arti_neuron.png}
	\caption[Caption for LOF]{The left-hand side of the image \footnotemark shows a biological neuron. It is a nerve cell that occurs in almost every animal. On the right-hand side is an artificial neuron. It sums up the stimuli of the previous neurons, applies an activation function to this sum and forwards the output of this function itself.}
	\label{fig:test1}
\end{figure}

\footnotetext{Source: Google.com}

\section{Adjustment of the edgeweights}
The algorithm for this is called \textit{backpropagation}. To train a network, the desired output must be known. Depending on the difference between the actual output and the desired output, the new weight of each edge is determined by derivation using the chain rule. The further away an edge is from the output, the less the edges weight gets updated. The chain rule approaches the desired output in very small steps. This bypasses the problem of Overfitting and prevents a neural network from learning the respective input by heart. Usually, many \textit{backpropagation}-Steps are required to train a network. For details see section \ref{subs:Example}


\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Figures/chap1/backpropagation_example.JPG}
	\caption{A simple neural network consisting of three layers. Neurons $\pi_1$ and $\pi_2$ are in the input layer. $\pi_3$ and $\pi_4$ make up the only hidden layer and $\pi_5$ is in the output layer. Each neuron sums up the input signals ($net\pi_i$), applies an activation function to it ($\sigma$) and forwards the output of this function as its own signal ($out\pi_i$). Mostly a sigmoid function is chosen as the activation function (right lower corner). The edgeweights are annotated with $\omega_i$.}
	\label{fig:backpropagation}
\end{figure}

\subsection{A simple claculation example (on Figure \ref{fig:backpropagation})}
\label{subs:Example}

For this example, we assume that the neurons (and the edges connected to it) labeled with $\pi_1$ and $\pi_2$ are in the input layer and the $\pi_5$ neuron is in the output layer. The layer where $\pi_3$ and $\pi_4$ are in, is called \textit{hidden layer}. For this example, the inputs and the edge-weights ($\omega_i$) are the following:
\begin{itemize}
	\item $Input_1$: 1
	\item $Input_2$: 0
	\item $\omega_i=\frac{i}{10}\forall i \in \{1,...,6\}$
\end{itemize}
First, let's calculate the output ($out\pi_5$) of the network:

\noindent\begin{minipage}{.5\linewidth}
	\begin{align*} 
		out\pi_1 &= \frac{1}{1+e^{-1}} = 0.731 \\ 
		out\pi_2 &= \frac{1}{1+e^{-0}} = 0.5 \\
		net\pi_3 &= 0.1*0.731+0.3*0.5 = 0.223 \\
		net\pi_4 &= 0.4*0.5+0.2*0.731 = 0.346
	\end{align*}
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\begin{align*} 
		out\pi_3 &= \frac{1}{1+e^{-0.223}} = 0.556 \\ 
		out\pi_4 &= \frac{1}{1+e^{-0.346}} = 0.586 \\
		net\pi_5 &= 0.5*0.556+0.6*0.586 = 0.630 \\
		out\pi_5 &= \frac{1}{1+e^{-0.630}} = 0.652
	\end{align*}
\end{minipage}\\
\\
\\
The output of the network is $0.652$, but we want it to be $0.1$. Since $\sigma^{-1}(0.1)=-ln(9)$ we expect both $\omega_5$ and $\omega_6$ to get smaller. For the \textit{backpropagation} we need an error-function. We choose the MSE (mean squared error). So $MSE=(0.652-0.1)^2 = 0.305$. We start the \textit{backpropagation} calculation with applying the chain rule to the derivative of the error with respect to the edge-weight which has to be updated:
\begin{equation*}
	\omega_i^* = \omega_i+\Delta \omega_i=\omega_i+\frac{\partial MSE}{\partial \omega_i}=\omega_i+\underbrace{\frac{\partial MSE}{\partial out\pi_5}*\frac{\partial out\pi_5}{\partial net\pi_5}}_{\delta_{\pi_5}}*\frac{\partial net\pi_5}{\partial \omega_i} \text{ for }i \in \{5,6\}
\end{equation*}
\begin{itemize}
	\item $\delta_{\pi_5}=\frac{\partial MSE}{\partial out\pi_5}*\frac{\partial out\pi_5}{\partial net\pi_5}=2*(0.652-0.1)*-1*0.652*(1-0.652)=-0.2505$
	\item $\omega_5^*=\omega_5+\delta_{\pi_5}*\frac{\partial net\pi_5}{\partial \omega_5}=\omega_5+\delta_{\pi_5}*out\pi_3=0.361$
	\item $\omega_6^*=\omega_6+\delta_{\pi_5}*\frac{\partial net\pi_5}{\partial \omega_6}=\omega_6+\delta_{\pi_5}*out\pi_4=0.453$
\end{itemize}
Now we move on with the $\Delta\omega_i$'s for the \textit{hidden layer}:
\begin{equation*}
	\frac{\partial E}{\partial \omega_i}=\underbrace{\underbrace{\frac{\partial MSE}{\partial out\pi_5}*\frac{\partial out\pi_5}{\partial net\pi_5}}_{\delta\pi_5}*\underbrace{\frac{\partial net\pi_5}{\partial out\pi_3}}_{\omega_5}*\frac{\partial out\pi_3}{\partial net\pi_3}}_{\delta_{\pi_3}}*\frac{\partial net\pi_3}{\partial \omega_i}\text{ for }i\in\{1,3\}
\end{equation*}
\begin{equation*}
	\frac{\partial E}{\partial \omega_i}=\underbrace{\underbrace{\frac{\partial MSE}{\partial out\pi_5}*\frac{\partial out\pi_5}{\partial net\pi_5}}_{\delta\pi_5}*\underbrace{\frac{\partial net\pi_5}{\partial out\pi_4}}_{\omega_6}*\frac{\partial out\pi_4}{\partial net\pi_4}}_{\delta_{\pi_4}}*\frac{\partial net\pi_4}{\partial \omega_i}\text{ for }i\in\{2,4\}
\end{equation*}

\noindent\begin{minipage}{.5\linewidth}
	\begin{align*} 
		\delta_{\pi_3} &= \delta_{\pi_5}*\omega_5^*out\pi_3*(1-out\pi_3)=-0.022\\ 
		\omega_1^* &= \omega_1+\delta_{\pi_3}*out\pi_1=0.084 \\
		\omega_3^* &= \omega_3+\delta_{\pi_3}*out\pi_2=0.289
	\end{align*}
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\begin{align*} 
		\delta_{\pi_4} &= \delta_{\pi_5}*\omega_6^*out\pi_4*(1-out\pi_4)=-0.028\\ 
		\omega_2^* &= \omega_2+\delta_{\pi_4}*out\pi_1=0.180 \\
		\omega_4^* &= \omega_4+\delta_{\pi_4}*out\pi_2=0.386
	\end{align*}
\end{minipage}\\
\\
If we now evaluate the network, we get a $out\pi_5$ of $0.613$. This is a very small change in absolute values because we have made only one iteration of the \textit{backpropagation} algorithm. However, the MSE improves from $0.305$ to $0.263$!

\section{The network used}
\label{sec:theFCN}
For this thesis, we used a pretrained fully convolutional network from \cite{nirkin2018_faceswap}. A Fully Convolutional Network (often called: FCN) is basically a CNN but with a modified architecture. An FCN does not have the fully connected layers which are usually found at the end of an CNN (see Figure \ref{fig:chap1:classicalCNN}). These layers would enable the Network to make decisions based on global information. A CNN for example can be used for classification. But for image analysis we want local information of the input image (we don't want to know if there is a face in the image, but where the face is in the image). Therefore a FCN uses only convolutional and pooling layers. In the whole Fully Convolutional Network only the following structure is repeated: One or more convolution layers and a pooling layer which downsamples the picture. This constellation is repeated several times.\\
\\
The assembly of the network used for this thesis follows the FCN-8s-VGG architecture with extensions of \cite{jlong}. The first part 'FCN' stands for 'Fully Convolutional Network', '8s' means that the result gets eight times upsampled (because of the pooling layers) and 'VGG' means the popular 16-layer network used by Oxford's Visual Geometry Group \cite{ksimonyan}. The original task of the network was to find the name of an object in an input image. The network could distinguish between 1000 different objects. Each cell in the final vector (1*1000 in size) was a boolean variable for one specific object.

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{Figures/fcn_1.png}
	\caption{In the lower-left corner, the 16 layers of the well known VGGnet are shown. In the top-right, you can see the meaning of the '8s' term of the FCN-Name. It means that the resulting image has to be 8x upsampled, to get an image which is in size equal to the input image. J. Long et al \cite{jlong} take the network and transform the fully connected layers into convolution layers (bottom-right).}
\end{figure}


\section{Related Work}
For our experiments a pretrained Fully Convolutional Network (FCN) of \cite{nirkin2018_faceswap} was used. They showed that even with a widespread network, you can do good segmentation and that the network does not have to be specially tailored for the future purpose. But the network must have been trained with a large enough data set. For more details about the used FCN and the architecture see section \ref{sec:theFCN}. They used the FCN for intra- and inter-subject face swapping on the Labeled Faces in the Wild (LFW) dataset and showed that intra-subject swapped faces remain as recognizable as before the swap and that in the inter-subject version better face swapping  leads to less perceptibility.\\
\\
They used a semi-supervised approach to produce training data in order to train the FCN. To produce large quantities of them, they used 2'043 face videos of the IARPA Janus CS2 dataset. To avoid searching for the face in every frame of the video they used motion queues which tracked the face given an initial segmentation based on \cite{grundmann} which enriched their training set to 9'818 images. To enlarge the collection of images, they rendered 3D Shapes of of various objects (e.g. sunglasses, hands) into existing images. Each occlusion adds 9'500 images to their training set.\\
\\
Egger et al. of the gravis group of the University of Basel propose a fully automated, probabilistic and occlusion-aware 3D morphable face model adaptation framework \cite{egger_paper}. These Methods use an iterative approach to generate the z-labels, namely to label each pixel whether it belongs to the face or to the background. Each label is described by its own model. This approach can handle multiple labels and differentiate between multiple occlusion types. For example face specific ones (eg. beards) and background. For updating the z-labels they use an algorithm which classifies a pixel based on the probabilities for each possible label.  But for our experiments we limited ourselves to two. We only need to distinguish between face (including skin and beard) and background.\\ 
\\
The algorithm does two things at the same time. In addition to creating a face mask (segmentation), it estimates the parameters of the popular Basel face Model to 3D reconstruct the given face. It's an EM-algorithm like method to solve two problems simultaneously. In the E-steps they updated the z-labels and in the M-steps they update the face model  parameters. For face model adaptation they apply a stochastic sampling strategy based on the Metropolis–Hastings algorithm (Markov Chain Monte Carlo). The likelihood is split up in a background model for each pixel ($X_{BG}$) and a foreground model ($X_{FG}$).
\[ 
\underbrace{p(\Theta |I)=p(I| \Theta)*p(\Theta)}_{\mathclap{\text{\parbox{4.5cm}{posterior brobability of face model parameters $\Theta$ given an image $I$}}}}\quad\quad\text{ with: }p(I|\Theta)=\prod_{X \in \text{pixels}}p(X_{BG}|\Theta)^{z-1}*p(X_{FG}|\Theta)^z
\]
Conventional approaches for segmentation often fail on important parts of the face such as the eyes, eyebrows or the oral region because of their strong variability in color and shape. The segmentation of Egger et al. has difficulties with these aspects too as Figure \ref{fig:iterations} shows. The algorithm starts with an initial guess and then alternating updates the Parameters $\Theta$ and the z-labels. From the updated Parameter Set (M-Step) the algorithm updates the z-labels (E-Step) and vice versa (see Figure \ref{fig:EGGER's_method}).\\
% Soll ich auch die 'stable illumination estimation' in ein chapter machen? Wenn ja, was ist das 'consensus set' ?

% The source for this table was this post: https://stackoverflow.com/questions/2771856/centering-text-horizontally-and-vertically-in-latex
% To add padding for the cell contents: https://tex.stackexchange.com/questions/31672/column-and-row-padding-in-tables
\begin{figure}[h]
	\begin{center}
		\newcolumntype{C}{>{\centering\arraybackslash} m{2.4cm} }  %# New column type
		\begin{tabular}{SC|SC|SC|SC|SC}
			 target image & 0 iterations & 10 iterations & 20 iterations & FCN\\ \hline
			\subfloat{\includegraphics[width=0.16\textwidth]{Figures/chap1/angie_original.png}} &
			\subfloat{\includegraphics[width=0.16\textwidth]{Figures/chap1/EGGER_Segmentation_Nr_0.png}} &
			\subfloat{\includegraphics[width=0.16\textwidth]{Figures/chap1/EGGER_Segmentation_Nr_9.png}} &
			\subfloat{\includegraphics[width=0.16\textwidth]{Figures/chap1/EGGER_Segmentation_Nr_19.png}}&
			\subfloat{\includegraphics[width=0.16\textwidth]{Figures/chap1/angie_FCN.png}} \\
		\end{tabular}
	\end{center}
	\caption{This picture shows the development of the labels after 0, 10 and 20 iterations. Striking in this sample image are not only the eyes as mentioned before but also the shadow of the nose, which is first segmented as a background. Only after a certain number of iterations these errors are partially recognized and provided with the correct label.}
	\label{fig:iterations}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{Figures/chap1/EGGER's_method.png}
	\caption[caption without footnote, for lof]{\footnotemark Algorithm overview: As input, we need a target image and fiducial points. We use the external Clandmark Library for automated fiducial point detection from still images (U\v ri\v c\' a\v r et al (2015) \cite{Uricar}). We start with an initial face model fit of our average face with a pose estimation. Then we perform a RANSAC-like robust illumination estimation for initialization of the segmentation label z and the illumination setting (for more details see Figure 9). Then our face model and the segmentation are simultaneously adapted to the target image I. The result is a set of face model parameters $\Theta$ and a segmentation into face and non-face regions. The presented target image is from the LFW face database (Huang et al. (2007))}
	\label{fig:EGGER's_method}
\end{figure}

\footnotetext{Figure \ref{fig:EGGER's_method} and it's description are one to one copyed from Fig.4 of the "Occlusion-aware  3D  Morphable  Models  and  and Illumination Prior for Face Image Analysis" paper of Egger et al.  \cite{egger_paper}}

An approach using convolutional neural networks to segment occluded faces has already been described by \cite{SaitoEtAl}. The big difference to our approach is that multiple frames are needed for the final segmentation. An other approach of \cite{MorelForster} of the University of Basel is to use random forests to detect facial-occlusions by hair to integrate occlusion into the fitting process in order to model uncertainty. Unlike us, they integrate the occlusion in the face likelihood of the evaluator.

\section{Expectations of the FCN}
Because \cite{nirkin2018_faceswap} trained the FCN on a very large and diverse dataset, we expect the network to succeed in the task of segmenting a variety of faces. They claim that a standard segmentation network is sufficient to segment as well as a network which was especially tailored for this task and outperforms state of the art methods for face segmentation. The testing of the network is done on real-life images, as well as on synthetically generated facial images which are occluded by an artificial object. We expect the FCN to do well on usual facial-occlusions (eg. hands, microphones) because it was trained on them and especially on the difficulties which are shown in Figure \ref{fig:iterations}.\\
\\
As already mentioned before, the approach of Egger et al. is iterative. Due to the high resolution of the Basel Face Model and the use of a software renderer they need up to 25 minutes per image. The performance of the code was less important to them.  According to Egger et al. the runtime for the segmentation is about 2 minutes. Nirkin et al. claim that the FCN they trained surpasses previous results in both accuracy and speed. Our measurements showed an average speed of 3.2 seconds per image, including the initialization of the FCN (on an Intel Xeon E7 v3/Xeon E5 v3/Core i7 DMI2 (rev 02)). So we expect the segmentations of the FCN to be qualitatively nearly as good as the segmentations of Egger et al. but to be much faster.